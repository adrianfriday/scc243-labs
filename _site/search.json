[
  {
    "objectID": "week3/week3-task.html",
    "href": "week3/week3-task.html",
    "title": "Week 3 worksheet",
    "section": "",
    "text": "The goal of today is to explore the ‘carbon footprint’ (related to embodied energy) of your own digital appliances and devices that you identified last week. Unlike direct energy use, the energy here is required at all stages of the manufacturing pipeline, so the carbon footprint relates to mining of raw materials, processing, transportation, manufacturing and all the other items that come together to make your products and get them in your hands.\n\n\nThe carbon footprint of each device is at best an estimate of the average or typical embodied energy, as the supply chains are very long and complex. They also differ between even similar products, and change over time (e.g. a market might source from different suppliers).\n\nYou may get differences in estimates, or only be able to get an estimate for a device that’s as similar as possible\nSometimes manufacturers and resellers will list the carbon footprint, there’s always questions we should ask about whether we can rely on the data - data can be old, contain errors, or it may be in their interest to ‘scope’ the estimate to make their products look good\nWe may have to start caring more about data quality, e.g. published peer reviews in reputable journals with ‘life cycle assessments’ (LCA) analysis are likely to be the most reliable sources (more on this in a later lecture)\n\nSince this is just a short lab task, approximate ‘ball park’ estimates are good enough to ‘get an idea’.\n\n\n\nTime available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available (git clone or git pull as needed to ensure you are up to date with the server), as before. In the ‘week3’ folder create a new markdown document called username-week3-labnotes.md, where ‘username’ is of course your username!\n\nFor each photo that you used last week, put a subheading that describes it\nLook up the carbon footprint of the device’s manufacture\nAdd a table. Bring over the annual energy use of the device based on your use pattern. Add a column converting this to CO2e by multiplying by a conversion factor (take care with your units!). In UK you can assume the energy mix is 0.207074 KgCO2e1\nAdd a column with the carbon footprint for the device\nNow add a row comparing the use footprint with the embodied footprint. Multiply the use footprint by the number of years you keep the device to get the total direct energy for that many years of use; calculate the ‘footprint per year’ of manufacturing footprint.\nYou should add a row for 1, 3, 5 and 10 years. In reality, some devices have shorter lifespans than this, but many (e.g. TVs, ovens) could last much longer.\nAdd a subsection ## Reflections with a short paragraph on your thoughts on how the various assets compare with one another, especially reflecting on the use phase related energy against the manufacturing or embodied energy.\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task.\n\n\n\n\nYou should have an appreciation of how the embodied energy varies with the appliances and devices you use most often\nYou should appreciate how embodied energy is amortised over the life of the device, and how this varies with longevity\nYou should see how use phase and embodied phase varies considerably across devices\nYou should be gaining some or enhancing your ‘carbon literacy’.\n\n\n\n\nHere are some useful digital resources to help. As before, very interested in which ones (or others) you actually use. Don’t forget, these are starting points, I’m not guaranteeing their quality! You should always have an open and enquring mind when it comes to data sources!!\n\nCO2 Everything - volunteer effort, I’ve not checked the data!\nCloud carbon footprint calculator\nInternet use footprint\nUniversity of Oxford IT provided figures - worth reading anyway\nFor fun, Small World Personal carbon footprint calculator\nEthical consumer guides, including technology"
  },
  {
    "objectID": "week3/week3-task.html#cautionary-note",
    "href": "week3/week3-task.html#cautionary-note",
    "title": "Week 3 worksheet",
    "section": "",
    "text": "The carbon footprint of each device is at best an estimate of the average or typical embodied energy, as the supply chains are very long and complex. They also differ between even similar products, and change over time (e.g. a market might source from different suppliers).\n\nYou may get differences in estimates, or only be able to get an estimate for a device that’s as similar as possible\nSometimes manufacturers and resellers will list the carbon footprint, there’s always questions we should ask about whether we can rely on the data - data can be old, contain errors, or it may be in their interest to ‘scope’ the estimate to make their products look good\nWe may have to start caring more about data quality, e.g. published peer reviews in reputable journals with ‘life cycle assessments’ (LCA) analysis are likely to be the most reliable sources (more on this in a later lecture)\n\nSince this is just a short lab task, approximate ‘ball park’ estimates are good enough to ‘get an idea’."
  },
  {
    "objectID": "week3/week3-task.html#task",
    "href": "week3/week3-task.html#task",
    "title": "Week 3 worksheet",
    "section": "",
    "text": "Time available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available (git clone or git pull as needed to ensure you are up to date with the server), as before. In the ‘week3’ folder create a new markdown document called username-week3-labnotes.md, where ‘username’ is of course your username!\n\nFor each photo that you used last week, put a subheading that describes it\nLook up the carbon footprint of the device’s manufacture\nAdd a table. Bring over the annual energy use of the device based on your use pattern. Add a column converting this to CO2e by multiplying by a conversion factor (take care with your units!). In UK you can assume the energy mix is 0.207074 KgCO2e1\nAdd a column with the carbon footprint for the device\nNow add a row comparing the use footprint with the embodied footprint. Multiply the use footprint by the number of years you keep the device to get the total direct energy for that many years of use; calculate the ‘footprint per year’ of manufacturing footprint.\nYou should add a row for 1, 3, 5 and 10 years. In reality, some devices have shorter lifespans than this, but many (e.g. TVs, ovens) could last much longer.\nAdd a subsection ## Reflections with a short paragraph on your thoughts on how the various assets compare with one another, especially reflecting on the use phase related energy against the manufacturing or embodied energy.\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task."
  },
  {
    "objectID": "week3/week3-task.html#learning-outcomes",
    "href": "week3/week3-task.html#learning-outcomes",
    "title": "Week 3 worksheet",
    "section": "",
    "text": "You should have an appreciation of how the embodied energy varies with the appliances and devices you use most often\nYou should appreciate how embodied energy is amortised over the life of the device, and how this varies with longevity\nYou should see how use phase and embodied phase varies considerably across devices\nYou should be gaining some or enhancing your ‘carbon literacy’."
  },
  {
    "objectID": "week3/week3-task.html#starting-points",
    "href": "week3/week3-task.html#starting-points",
    "title": "Week 3 worksheet",
    "section": "",
    "text": "Here are some useful digital resources to help. As before, very interested in which ones (or others) you actually use. Don’t forget, these are starting points, I’m not guaranteeing their quality! You should always have an open and enquring mind when it comes to data sources!!\n\nCO2 Everything - volunteer effort, I’ve not checked the data!\nCloud carbon footprint calculator\nInternet use footprint\nUniversity of Oxford IT provided figures - worth reading anyway\nFor fun, Small World Personal carbon footprint calculator\nEthical consumer guides, including technology"
  },
  {
    "objectID": "week3/week3-task.html#footnotes",
    "href": "week3/week3-task.html#footnotes",
    "title": "Week 3 worksheet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFigure from 2023, Department for Energy Security and Net Zero.↩︎"
  },
  {
    "objectID": "week5/week5-task.html",
    "href": "week5/week5-task.html",
    "title": "Week 5 worksheet",
    "section": "",
    "text": "The goal of today is to explore the effect of offloading computation from the CPU to the GPU, exploring the maximum potential for reducing the energy implicated with doing so. The GPU in most lab machines on campus is the onboard Intel graphics which is considerably lower power draw than a dedicated GPU found in most gaming machines and high end AI or graphics machines. Recall, that processors increasingly have other co-processors, such as Intel’s NPU, for accelerating operations for machine learning - which also might perform specialist parallel instructions with good compute/energy performance. You should keep this in mind when generalising from your findings!\nWe will use the same measurement setup as last week, so I again strongly recommend you do this on the lab machine, and not on your home machine or laptop. It does not work on the VM with virtualised hardware.\n\n\nReminder:\n\nYou should aim to take good, repeatable measurements, minimising interference from other tasks on the system.\nYou should run each test multiple times to get several measurements and take a mean and standard deviation.\nMeasuring the GPU is more difficult, so you will need to look more closely at the numbers coming back from the profiling script to isolate the CPU cores and GPU from the rest, see below.\nI’d suggest running each test 5 times, providing all of the readings in your report markdown fie.\nYou should do a ‘warmup’ run before taking your measurements, not least because this task downloads a large data file at startup. Be careful not to include the download and data loading time in your benchmark results. Take care also to remove or not include the data file in your git repo!\n\nWe’re going to use a test which forecasts data points from a linear regression using python for this task which utilises the scitkit-learn package and GPU extension by Intel).\n\n\n\nThe profiler script will give you a block of output like this:\nPerformance Metrics:\nExecution time: 6402.042 ms (6402042057 ns)\nCPU time (user): 16.690 seconds\nCPU time (system): 0.680 seconds\n\nCPU Power Usage (RAPL):\n  intel-rapl:0: 133020839 µJ (133.021 J)\n  intel-rapl:0:0: 123113210 µJ (123.113 J)\n  intel-rapl:0:1: 7812 µJ (0.008 J)\nIf you’re curious, read about user and system time in this helpful answer to this stackoverflow question.\nNote that the intel-rapl: corresponds to different MSRs (module specific registers) on your Intel processor. These are normally:\n\n\n\n\n\n\n\n\nChannel\nValue\nMeaning\n\n\n\n\nintel-rapl:0:\n133020839 µJ (133.021 J)\nOverall ‘package’ energy (all parts of the CPU package, including CPU and GPU)\n\n\nintel-rapl:0:0:\n123113210 µJ (123.113 J)\nChannel 0 is the ‘CPU cores’ part of the package\n\n\nintel-rapl:0:1:\n7812 µJ (0.008 J)\n“Normally” the GPU\n\n\n\nNote that 0.008 Joules is tiny, so probably more significant in interpreting your results is the difference between the package and CPU channels and estimating ‘what is saved’ by the GPU by taking workload from the processor between test variants. The energy saved should really be not just the difference in load, but also the reduced execution time. You’ll need to take this into account and explain your working out in your markdown report.\n\n\n\nTime available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available. In the ‘week5’ folder create a new markdown document called username-week5-labnotes.md.\n\nRun the terminal, and switch into a python virtual environment for our course1\nsource /usr/local/lib/python-venv/scc-243/bin/activate\nThis should provide all the python packages we’re going to use without the need to download and install them.\nDownload the profiler.py script we used last week (or copy it from last week’s folder - it hasn’t changed.)\nDownload the two test files test-cpu.py and test-gpu.py. This is the same code, the only difference being that with test-cpu.py all the calculation is done on the CPU cores.\nRun the test case to ‘warm up’ the system (load packages and data files for the first time).\npython test-cpu.py\nRun each test case at least 5 times, noting down the test case, energy and time taken for each. e.g. using:\n./profiler.py \"python test-cpu.py\"\nImportant note: the profiler.py swallows the output from the task, so I’d recommend running the task to verify it’s running ok, before running under the energy profiler. Especially, if the task completion time and energy used is suspiciously small!\nDocument your experiment in markdown format, using markdown tables as last week. Add an introductory paragraph on what you plan to measure and your ‘method statement’ of how you’ve conducted your tests. Take care to explain how you are calculating the energy saved.\nFor each test case, add a subsection with the goal of the test, the command line you’ve used and a table with the results.\nThe table should have the results of each test run, how much time it took and how many Joules of energy and total watt/hours.\nFinish the table with the average time taken and standard deviation.\nAdd a subsection called ## Reflections which summarises what you’ve found, and any personal observations on the number of Watt’s associated with the computations. How much energy was saved by offloading from CPU to GPU and speeding up the task? If you ran this task for an entire day, week, month or year how many kWh would be used for CPU and GPU variants?\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task. Take care not to add the ‘data’ folder to your git repository - it contains an enormous data file that you don’t need to keep! You can remove it with git rm -r data or on scc-source if you need to.\n\n\n\n\nYou should have an appreciation of how energy varies with CPU vs. GPU computation\nShould reinforce your expirimental method, i.e. how to take measurements, calculate a difference in energy saved and report them in a systematic way\nA ‘ready reckoning’ of the bounds for energy saving due to reducing load on the CPU by shifting workloads to the GPU\nAn appreciation of how efficient GPU cores are for offloading parallel mathematical tasks (e.g. vector matrix operations)"
  },
  {
    "objectID": "week5/week5-task.html#taking-good-measurements",
    "href": "week5/week5-task.html#taking-good-measurements",
    "title": "Week 5 worksheet",
    "section": "",
    "text": "Reminder:\n\nYou should aim to take good, repeatable measurements, minimising interference from other tasks on the system.\nYou should run each test multiple times to get several measurements and take a mean and standard deviation.\nMeasuring the GPU is more difficult, so you will need to look more closely at the numbers coming back from the profiling script to isolate the CPU cores and GPU from the rest, see below.\nI’d suggest running each test 5 times, providing all of the readings in your report markdown fie.\nYou should do a ‘warmup’ run before taking your measurements, not least because this task downloads a large data file at startup. Be careful not to include the download and data loading time in your benchmark results. Take care also to remove or not include the data file in your git repo!\n\nWe’re going to use a test which forecasts data points from a linear regression using python for this task which utilises the scitkit-learn package and GPU extension by Intel)."
  },
  {
    "objectID": "week5/week5-task.html#understanding-intel-rapl-channels",
    "href": "week5/week5-task.html#understanding-intel-rapl-channels",
    "title": "Week 5 worksheet",
    "section": "",
    "text": "The profiler script will give you a block of output like this:\nPerformance Metrics:\nExecution time: 6402.042 ms (6402042057 ns)\nCPU time (user): 16.690 seconds\nCPU time (system): 0.680 seconds\n\nCPU Power Usage (RAPL):\n  intel-rapl:0: 133020839 µJ (133.021 J)\n  intel-rapl:0:0: 123113210 µJ (123.113 J)\n  intel-rapl:0:1: 7812 µJ (0.008 J)\nIf you’re curious, read about user and system time in this helpful answer to this stackoverflow question.\nNote that the intel-rapl: corresponds to different MSRs (module specific registers) on your Intel processor. These are normally:\n\n\n\n\n\n\n\n\nChannel\nValue\nMeaning\n\n\n\n\nintel-rapl:0:\n133020839 µJ (133.021 J)\nOverall ‘package’ energy (all parts of the CPU package, including CPU and GPU)\n\n\nintel-rapl:0:0:\n123113210 µJ (123.113 J)\nChannel 0 is the ‘CPU cores’ part of the package\n\n\nintel-rapl:0:1:\n7812 µJ (0.008 J)\n“Normally” the GPU\n\n\n\nNote that 0.008 Joules is tiny, so probably more significant in interpreting your results is the difference between the package and CPU channels and estimating ‘what is saved’ by the GPU by taking workload from the processor between test variants. The energy saved should really be not just the difference in load, but also the reduced execution time. You’ll need to take this into account and explain your working out in your markdown report."
  },
  {
    "objectID": "week5/week5-task.html#task",
    "href": "week5/week5-task.html#task",
    "title": "Week 5 worksheet",
    "section": "",
    "text": "Time available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available. In the ‘week5’ folder create a new markdown document called username-week5-labnotes.md.\n\nRun the terminal, and switch into a python virtual environment for our course1\nsource /usr/local/lib/python-venv/scc-243/bin/activate\nThis should provide all the python packages we’re going to use without the need to download and install them.\nDownload the profiler.py script we used last week (or copy it from last week’s folder - it hasn’t changed.)\nDownload the two test files test-cpu.py and test-gpu.py. This is the same code, the only difference being that with test-cpu.py all the calculation is done on the CPU cores.\nRun the test case to ‘warm up’ the system (load packages and data files for the first time).\npython test-cpu.py\nRun each test case at least 5 times, noting down the test case, energy and time taken for each. e.g. using:\n./profiler.py \"python test-cpu.py\"\nImportant note: the profiler.py swallows the output from the task, so I’d recommend running the task to verify it’s running ok, before running under the energy profiler. Especially, if the task completion time and energy used is suspiciously small!\nDocument your experiment in markdown format, using markdown tables as last week. Add an introductory paragraph on what you plan to measure and your ‘method statement’ of how you’ve conducted your tests. Take care to explain how you are calculating the energy saved.\nFor each test case, add a subsection with the goal of the test, the command line you’ve used and a table with the results.\nThe table should have the results of each test run, how much time it took and how many Joules of energy and total watt/hours.\nFinish the table with the average time taken and standard deviation.\nAdd a subsection called ## Reflections which summarises what you’ve found, and any personal observations on the number of Watt’s associated with the computations. How much energy was saved by offloading from CPU to GPU and speeding up the task? If you ran this task for an entire day, week, month or year how many kWh would be used for CPU and GPU variants?\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task. Take care not to add the ‘data’ folder to your git repository - it contains an enormous data file that you don’t need to keep! You can remove it with git rm -r data or on scc-source if you need to."
  },
  {
    "objectID": "week5/week5-task.html#learning-outcomes",
    "href": "week5/week5-task.html#learning-outcomes",
    "title": "Week 5 worksheet",
    "section": "",
    "text": "You should have an appreciation of how energy varies with CPU vs. GPU computation\nShould reinforce your expirimental method, i.e. how to take measurements, calculate a difference in energy saved and report them in a systematic way\nA ‘ready reckoning’ of the bounds for energy saving due to reducing load on the CPU by shifting workloads to the GPU\nAn appreciation of how efficient GPU cores are for offloading parallel mathematical tasks (e.g. vector matrix operations)"
  },
  {
    "objectID": "week5/week5-task.html#footnotes",
    "href": "week5/week5-task.html#footnotes",
    "title": "Week 5 worksheet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAbout python virtual environments.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SCC243 Sustainable Computing Labs",
    "section": "",
    "text": "Weekly lab tasks for SCC.243 Sustainable Computing."
  },
  {
    "objectID": "index.html#weekly-tasks-build-up-a-portfolio-of-assessment",
    "href": "index.html#weekly-tasks-build-up-a-portfolio-of-assessment",
    "title": "SCC243 Sustainable Computing Labs",
    "section": "Weekly tasks build up a portfolio of assessment",
    "text": "Weekly tasks build up a portfolio of assessment\nEach week there is:\n\na task to do in lab\nsometimes I’ll set a recommended essential reading\nthere may be additional tasks (such as short pieces of reflective writing, in blog post format, especially at the start and end of the course)\n\nAll the writing will be conducted in markdown (.md files) and submitted via a private github repository that is shared with the course convenor ‘@friday’, that you create in the week 1 lab!\nThe labs are anticipated to be as follows:\n\n\n\nWeek\nLab\nSee details (link)\n\n\n\n\n1\nSetup research task, github; opening reflective statement\nweek1\n\n\n2\nMy digital footprint survey/ photos\nweek2\n\n\n3\nWorkload footprint estimation\nweek3\n\n\n4\nMeasurement lab\nweek4\n\n\n5\nGreen AI measurement lab\nweek5\n\n\n6\nTopic & reading selection\nweek6\n\n\n7\nAnnotated references\nweek7\n\n\n8\nScript development\nweek8\n\n\n9\nRecord/edit podcast\n\n\n\n10\nEdit, write abstract, submit; closing reflective statement (w. ref to other courses/position on learning)\n\n\n\n\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "week7/week7-task.html",
    "href": "week7/week7-task.html",
    "title": "Week 7 worksheet",
    "section": "",
    "text": "Weeks 6-10 are to be used to produce a 10 minute audio podcast exploring interesting and contemporary digital sustainability research questions. You should use the labs to discuss your core research question, approach, critical selection of evidence and materials. Plus to learn and practice using the audio editing and recording software (Audacity).\nEach week has a short written deliverable, and a submission is not complete without all the minor deliverables.\n\n\nYou should have selected the topic of your podcast. If you’re in any doubt about it, then please discuss with the staff in your lab.\nToday’s goal is to find evidence that lets you explore positions, identify quotes, figures or trends to expand on your argument. This could either be claim and counter-claim, or narratives and debate that you’ve found in the literature.\n\nYou should create week7-notes.md in your week7 folder.\nYour goal today is to create an annotated bibliography, so start with this main markdown heading. The annotated bibliography is a full reference citation for each source, or URL as appropriate.\nFor example, a full citation to Fretag et al. looks like:\nFreitag, C., Berners-Lee, M., Widdicks, K., Knowles, B., Blair, G. S., and Friday, A. The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations. Patterns. 2, 9 (2021), 100340. doi: 10.1016/j.patter.2021.100340\nThen you should add 3 things: 1. a sentence on why you’ve chosen it. 2. a sentence or two on why you trust it, clearly stating the criteria you’ve explored to evidence this trust. Then 3. a third sentence on the threat to validity, i.e. is there any reason you don’t trust it, or any limitation we should be concerned about.\nRemember to: add, commit and push any new files you’ve created.\n\nNote that just providing a series of links or references without the explanations suggested is not acceptable. I would expect a mix of content sources, not just a list of links, with clear evidence that you’ve chosen thoughtfully and carefully.\n\n\nGoogle Scholar is a valuable resource for looking up papers or particular authors. You will also find lots of good material via onesearch.\nTo read a paper quickly, I recommend reading the abstract, skim reading the introduction, then paging through to the discussion and conclusions. A well written abstract and conclusion should capture succinctly the contribution of the paper. You may find good quotes to use in the ‘findings sections’ and the headings are sometimes informative as to the key messages.\nYou may find authoritative people appear in talks, sometimes on YouTube, interviewed in podcasts or on their research group’s website. It’s worth looking them up - you may find some good audio you can take clips from. Don’t forget to note down any copyright or license constraints.\n\n\n\nReminder as we said last week, there are many different forms of evidence, and some sources can be trusted more than others. You should be clear what is trustworthy and what might be compromised by vested interests, political or business ideology; or are just unsubstantiated claims, deliberate misinformation, or clickbait.\n\nPeer reviewed and respectable papers in good journals are probably more trusted, but in some areas, it is also possible for a paper to be respected and respectable, and still come to false conclusions (especially in low quality or pay to publish journals) - note some papers have already been found in journals that were created by ChatGPT, and no one caught it! Authors can be biased (for example, if the author has something to gain from arguing a certain position) - consider who they work for, whether they have credibility, and whether they are likely independent and objective.\nGrey literature sources such as blogs and social media posts will be prevalent, take even greater care in taking the conclusions and argument at face value. Look closely and critically at their sources. Who are they, do they have appropriate knowledge and experience? From what you know about energy and GHG, do their numbers seem in a credible range, or even physically possible?\nGenerative AI results need to be looked at especially carefully (i.e. click through to the source of anything it links to and check how authoritative it is!) Remember, that AI models hallucinate confidently even if they don’t have much data!\n\n\n\n\n\n\nThe podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week7/week7-task.html#task-2-identify-and-evaluate-evidence",
    "href": "week7/week7-task.html#task-2-identify-and-evaluate-evidence",
    "title": "Week 7 worksheet",
    "section": "",
    "text": "You should have selected the topic of your podcast. If you’re in any doubt about it, then please discuss with the staff in your lab.\nToday’s goal is to find evidence that lets you explore positions, identify quotes, figures or trends to expand on your argument. This could either be claim and counter-claim, or narratives and debate that you’ve found in the literature.\n\nYou should create week7-notes.md in your week7 folder.\nYour goal today is to create an annotated bibliography, so start with this main markdown heading. The annotated bibliography is a full reference citation for each source, or URL as appropriate.\nFor example, a full citation to Fretag et al. looks like:\nFreitag, C., Berners-Lee, M., Widdicks, K., Knowles, B., Blair, G. S., and Friday, A. The real climate and transformative impact of ICT: A critique of estimates, trends, and regulations. Patterns. 2, 9 (2021), 100340. doi: 10.1016/j.patter.2021.100340\nThen you should add 3 things: 1. a sentence on why you’ve chosen it. 2. a sentence or two on why you trust it, clearly stating the criteria you’ve explored to evidence this trust. Then 3. a third sentence on the threat to validity, i.e. is there any reason you don’t trust it, or any limitation we should be concerned about.\nRemember to: add, commit and push any new files you’ve created.\n\nNote that just providing a series of links or references without the explanations suggested is not acceptable. I would expect a mix of content sources, not just a list of links, with clear evidence that you’ve chosen thoughtfully and carefully.\n\n\nGoogle Scholar is a valuable resource for looking up papers or particular authors. You will also find lots of good material via onesearch.\nTo read a paper quickly, I recommend reading the abstract, skim reading the introduction, then paging through to the discussion and conclusions. A well written abstract and conclusion should capture succinctly the contribution of the paper. You may find good quotes to use in the ‘findings sections’ and the headings are sometimes informative as to the key messages.\nYou may find authoritative people appear in talks, sometimes on YouTube, interviewed in podcasts or on their research group’s website. It’s worth looking them up - you may find some good audio you can take clips from. Don’t forget to note down any copyright or license constraints.\n\n\n\nReminder as we said last week, there are many different forms of evidence, and some sources can be trusted more than others. You should be clear what is trustworthy and what might be compromised by vested interests, political or business ideology; or are just unsubstantiated claims, deliberate misinformation, or clickbait.\n\nPeer reviewed and respectable papers in good journals are probably more trusted, but in some areas, it is also possible for a paper to be respected and respectable, and still come to false conclusions (especially in low quality or pay to publish journals) - note some papers have already been found in journals that were created by ChatGPT, and no one caught it! Authors can be biased (for example, if the author has something to gain from arguing a certain position) - consider who they work for, whether they have credibility, and whether they are likely independent and objective.\nGrey literature sources such as blogs and social media posts will be prevalent, take even greater care in taking the conclusions and argument at face value. Look closely and critically at their sources. Who are they, do they have appropriate knowledge and experience? From what you know about energy and GHG, do their numbers seem in a credible range, or even physically possible?\nGenerative AI results need to be looked at especially carefully (i.e. click through to the source of anything it links to and check how authoritative it is!) Remember, that AI models hallucinate confidently even if they don’t have much data!"
  },
  {
    "objectID": "week7/week7-task.html#learning-outcomes",
    "href": "week7/week7-task.html#learning-outcomes",
    "title": "Week 7 worksheet",
    "section": "",
    "text": "The podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week1/week1-task.html",
    "href": "week1/week1-task.html",
    "title": "Part 2: Week 1 worksheet",
    "section": "",
    "text": "The goal of today is to explore some of the world’s digital data infrastructure, and specifically, wired and wireless network connections and data centres.\nTime available: 1 hour:\n\nSetup git as per these instructions - 10 minutes\nGet started with markdown, see how to use markdown - 10 minutes\nThis research task - ~30 minutes\nBy the end of the week, please add a further short markdown document ‘username-me.md’ to your week1 folder with no more than 200 words on i. how significant you think ICT is in terms of climate change (impacts, benefits); and ii. 3 things (bullets) on what you’d like to learn on this course - this is your quick thoughts so I understand where we’re coming from, not a referenced argument or essay!\nDon’t forget to add any files you create, commit and push your repo at the end of the lab\n\n\n\nYou should create a lab notebook file for today to keep track of:\n\ndigital resources you’ve found, service provider/owner and statistics (link or data centre capacities, and ideally maximum energy demand)\nYou can choose a location to explore, this could be Lancaster or where you’re from, or a city or location worldwide you’d like to visit - on the proviso that some digital infrastructure is included (i.e. it’d be easy to pick somewhere remote with no infrastructure, and this isn’t an excuse to do so!)\nYou can choose to focus on data centres, networks, wireless cell towers, or any other regional computational assets you can find. I would recommend focusing on exploring one of these in detail, but you could pick more than one if time allows.\nThe task should be time bounded, you should not spend much more than 30 minutes of concentrated time. I would expect you to find 3-5 assets in detail in this time.\n\n\n\n\n\nCall your lab notes something like ‘username-week1-labnotes.md’, where username is your login id (mine would be friday). Create it inside your week1 repository folder, i.e. the same folder as this file.\nAdd a title, sentence about what asset or assets and region you’ve chosen to focus on. Add a subsection for each asset type, e.g. ‘## Data centres in Manchester’. See how to use markdown for hints.\nFor each asset, add it’s title, a link (URL) to information about it (capacity, energy demand, owner) and the search engine/map you used to find it. I’d recommend a markdown table for each asset. If you want to screenshot or include graphical snapshots of interesting features you find, then please feel free. Save any images you link to from your markdown file into the same week1 folder. I’d recommend jpg or png file formats for images (i.e. common formats supported by github’s markdown renderer).\nAdd a subsection at the end called ‘Reflections’. Write a paragraph under this with your overall thoughts on what you’ve found and observed, and ideally whether it has surprised you or confirmed what you already knew. How sustainable do you think it is?\nRemember to ‘git add’ any files you create, including any image files (or they won’t be committed). You can check what’s tracked in your report by using 'git status' in the terminal. Commit and push your repo at the end of the task (as a minimum). You can commit more often than this should you wish or continue after the allocated lab time.\n\n\n\n\n\nHow much provision is there in the region?\nHow does provision vary and relate to geography (power, water, coast, built environment, roads, rail), and population?\nIs there duplicate provision (e.g. capacity owned by more than one operator but serving the same region)? How many operators are there?\nAre there different levels of service (scales of datacentre, network capacity, coverage and data rates) to different regions?\nDoes the digital infrastructure follow any other built infrastructures (road, rail, waterways, coasts)\nIf possible, has this changed over time? (e.g. what was there 3, 5, 10 years ago)?\n\n\n\n\n\nYou should have an appreciation of regional digital infrastructure\nYou should have a better understanding of the distribution and clustering of digital services with geography\nYou should appreciate the relationship between different digital services, population areas in relation to other non-digital infrastructures\n\n\n\n\nHere are some useful digital starting points. Don’t be afraid to look for your own. I’m definitely interested in what you find and how you find it, and also what you’re thinking about it!\n\n\n\nData centre map\nColocation datacentre map\nCloud infastructure map\nArcGis US datacentres\n5G wireless map\n3G, 4G, 5G map\nBroadband UK map\nInternet exchange map\nTeleGeography transatlantic fibre map\n\n\n\n\n\nMains power map\nRailways in UK map\nOpen streetmap\nUSGS satellite maps"
  },
  {
    "objectID": "week1/week1-task.html#task",
    "href": "week1/week1-task.html#task",
    "title": "Part 2: Week 1 worksheet",
    "section": "",
    "text": "You should create a lab notebook file for today to keep track of:\n\ndigital resources you’ve found, service provider/owner and statistics (link or data centre capacities, and ideally maximum energy demand)\nYou can choose a location to explore, this could be Lancaster or where you’re from, or a city or location worldwide you’d like to visit - on the proviso that some digital infrastructure is included (i.e. it’d be easy to pick somewhere remote with no infrastructure, and this isn’t an excuse to do so!)\nYou can choose to focus on data centres, networks, wireless cell towers, or any other regional computational assets you can find. I would recommend focusing on exploring one of these in detail, but you could pick more than one if time allows.\nThe task should be time bounded, you should not spend much more than 30 minutes of concentrated time. I would expect you to find 3-5 assets in detail in this time."
  },
  {
    "objectID": "week1/week1-task.html#written-record",
    "href": "week1/week1-task.html#written-record",
    "title": "Part 2: Week 1 worksheet",
    "section": "",
    "text": "Call your lab notes something like ‘username-week1-labnotes.md’, where username is your login id (mine would be friday). Create it inside your week1 repository folder, i.e. the same folder as this file.\nAdd a title, sentence about what asset or assets and region you’ve chosen to focus on. Add a subsection for each asset type, e.g. ‘## Data centres in Manchester’. See how to use markdown for hints.\nFor each asset, add it’s title, a link (URL) to information about it (capacity, energy demand, owner) and the search engine/map you used to find it. I’d recommend a markdown table for each asset. If you want to screenshot or include graphical snapshots of interesting features you find, then please feel free. Save any images you link to from your markdown file into the same week1 folder. I’d recommend jpg or png file formats for images (i.e. common formats supported by github’s markdown renderer).\nAdd a subsection at the end called ‘Reflections’. Write a paragraph under this with your overall thoughts on what you’ve found and observed, and ideally whether it has surprised you or confirmed what you already knew. How sustainable do you think it is?\nRemember to ‘git add’ any files you create, including any image files (or they won’t be committed). You can check what’s tracked in your report by using 'git status' in the terminal. Commit and push your repo at the end of the task (as a minimum). You can commit more often than this should you wish or continue after the allocated lab time."
  },
  {
    "objectID": "week1/week1-task.html#things-to-explore",
    "href": "week1/week1-task.html#things-to-explore",
    "title": "Part 2: Week 1 worksheet",
    "section": "",
    "text": "How much provision is there in the region?\nHow does provision vary and relate to geography (power, water, coast, built environment, roads, rail), and population?\nIs there duplicate provision (e.g. capacity owned by more than one operator but serving the same region)? How many operators are there?\nAre there different levels of service (scales of datacentre, network capacity, coverage and data rates) to different regions?\nDoes the digital infrastructure follow any other built infrastructures (road, rail, waterways, coasts)\nIf possible, has this changed over time? (e.g. what was there 3, 5, 10 years ago)?"
  },
  {
    "objectID": "week1/week1-task.html#learning-outcomes",
    "href": "week1/week1-task.html#learning-outcomes",
    "title": "Part 2: Week 1 worksheet",
    "section": "",
    "text": "You should have an appreciation of regional digital infrastructure\nYou should have a better understanding of the distribution and clustering of digital services with geography\nYou should appreciate the relationship between different digital services, population areas in relation to other non-digital infrastructures"
  },
  {
    "objectID": "week1/week1-task.html#starting-points",
    "href": "week1/week1-task.html#starting-points",
    "title": "Part 2: Week 1 worksheet",
    "section": "",
    "text": "Here are some useful digital starting points. Don’t be afraid to look for your own. I’m definitely interested in what you find and how you find it, and also what you’re thinking about it!\n\n\n\nData centre map\nColocation datacentre map\nCloud infastructure map\nArcGis US datacentres\n5G wireless map\n3G, 4G, 5G map\nBroadband UK map\nInternet exchange map\nTeleGeography transatlantic fibre map\n\n\n\n\n\nMains power map\nRailways in UK map\nOpen streetmap\nUSGS satellite maps"
  },
  {
    "objectID": "week8/week8-task.html",
    "href": "week8/week8-task.html",
    "title": "Week 8 worksheet",
    "section": "",
    "text": "Weeks 6-10 are to be used to produce a 10 minute audio podcast exploring interesting and contemporary digital sustainability research questions. You should use the labs to discuss any further reflections on your choice of core research question, and critically explore the evidence for and against that you’ve identified.\nEspecially important is to clear up any concerns you may have as to whether the evidence is sufficiently good, and whether your annotated bibliography from last week, is ok.\n\n\n\n\n\n\nReminder on AI use policy\n\n\n\nAmber: See AI and academic practice framework. AI can be used to help you search for evidence sources and explore opinions on topics. AI should not be used to generate the script or podcast audio.\n\n\n\n\nYou should have selected the topic of your podcast and found evidence for arguing your position. Please discuss as necessary any concerns about this with the staff in your lab.\nToday’s goal is create a script that structures your podcast and draws the links from the points you plan to make back to the evidence sources, and/ or sources of media you plan to incorporate (own recording, quote from source X, from recording Y from A time to B time).\nThis is not an exercise in being able to use generative AI to create a podcast. e.g. descript.\nBut, do think about the structure of your podcast. Podcasts usually have a ‘shape’ or overall structure to them:\n\nHow to write a podcast script\nSome nice ideas in this reddit post\n\nYour goal is to introduce your question in an engaging way, bring in the claims and counter claims to explore the issue, then close with a good point to end on. You could also think about royalty free background, intro or outro music to incorporate. There is no need for a message from your sponsor :)\n\nYou should create week8-notes.md in your week8 folder.\nYour goal today is to create a podcast script, so start with this main markdown heading. It’s not a verbatim text of what you plan to say, rather:\n\nsection headings for the key structural elements\na running order for what you plan to talk about, facts, quotes or media elements you plan to include\nlink outs to sources for each of these elements, noting any copyright issues\n\nA ‘shooting list’ of any audio resources you need to record, interviews you need to conduct, or music items you need to identify\nRemember to: add, commit and push any new files you’ve created.\n\nNote again: this is your framing of your podcast, and using one of the online templates verbatim or gen AI script writing tools is not acceptable and is academic malpractice.\n\n\n\n\nThe podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week8/week8-task.html#task-3-script-your-podcast",
    "href": "week8/week8-task.html#task-3-script-your-podcast",
    "title": "Week 8 worksheet",
    "section": "",
    "text": "You should have selected the topic of your podcast and found evidence for arguing your position. Please discuss as necessary any concerns about this with the staff in your lab.\nToday’s goal is create a script that structures your podcast and draws the links from the points you plan to make back to the evidence sources, and/ or sources of media you plan to incorporate (own recording, quote from source X, from recording Y from A time to B time).\nThis is not an exercise in being able to use generative AI to create a podcast. e.g. descript.\nBut, do think about the structure of your podcast. Podcasts usually have a ‘shape’ or overall structure to them:\n\nHow to write a podcast script\nSome nice ideas in this reddit post\n\nYour goal is to introduce your question in an engaging way, bring in the claims and counter claims to explore the issue, then close with a good point to end on. You could also think about royalty free background, intro or outro music to incorporate. There is no need for a message from your sponsor :)\n\nYou should create week8-notes.md in your week8 folder.\nYour goal today is to create a podcast script, so start with this main markdown heading. It’s not a verbatim text of what you plan to say, rather:\n\nsection headings for the key structural elements\na running order for what you plan to talk about, facts, quotes or media elements you plan to include\nlink outs to sources for each of these elements, noting any copyright issues\n\nA ‘shooting list’ of any audio resources you need to record, interviews you need to conduct, or music items you need to identify\nRemember to: add, commit and push any new files you’ve created.\n\nNote again: this is your framing of your podcast, and using one of the online templates verbatim or gen AI script writing tools is not acceptable and is academic malpractice."
  },
  {
    "objectID": "week8/week8-task.html#learning-outcomes",
    "href": "week8/week8-task.html#learning-outcomes",
    "title": "Week 8 worksheet",
    "section": "",
    "text": "The podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week6/week6-task.html",
    "href": "week6/week6-task.html",
    "title": "Week 6 worksheet",
    "section": "",
    "text": "Use of AI\n\n\n\nAmber: See AI and academic practice framework. AI can be used to help you search for evidence sources and explore opinions on topics. AI should not be used to generate the script or podcast audio. You should state clearly in a section in your reflective report for the podcast in week 10, how AI has been used. Take great care, as AI can easily hallucinate evidence and references that don’t in fact exist!. I highly recommend this short online module on what LLMs can and can’t do, it’s really interesting, informative and thought provoking!\n\n\nWeeks 6-10 are to be used to produce a 10 minute audio podcast exploring interesting and contemporary digital sustainability research questions. You should use the labs to discuss your core research question, approach, critical selection of evidence and materials. Plus to learn and practice using the audio editing and recording software (Audacity).\nEach week has a short written deliverable, and a submission is not complete without all the minor deliverables.\nThe final submission by week 10 is the audio podcast file via a moodle submission point and and description summarising what the podcast does, a list of recorded materials/ sources if any are included, any copyright implications that would prevent its wide publication/ reuse, and a #Reflection section on what you’ve learnt via the repo (week10 folder).\n\n\nThe topic of your podcast should explore a contemporary topic or question in ICT and sustainability. This can be your evaluation of the evidence available, but should be backed up by evidence you have identified and evaluated.\nExample ideas (but not limited to) might be:\n\nShould we be worried by AI’s energy demand?\nWhat is driving growth in ICT and can it be stopped?\nWhat is the role of ICT in helping us integrate renewables?\nShould we trust climate change advice from a chatbot?\nDoes a footballer’s tweet use the same energy as a rocket?\nWhat is stopping my phone or PC lasting 10 years?\nCan ICT be sustainable?\n\nI very much expect you to come up with your own ideas. But a key aspect is that you can find enough sources of high quality material and evidence to support an argument in a podcast.\n\nYou should create week6-notes.md in your week6 folder. Add a heading # Research question. Provide your research question and a brief justification paragraph of why you think it will make an interesting podcast.\nAdd a heading # Supporting material. List links to sources you’ve identified which provide evidence for and against the central argument of your research question. Your goal is to provide a well reasoned and logical argument based on objective and rational analysis. Unsupported opinion, unevidenced rants, sales/ greenwash will not gain good marks!\nRemember to: add, commit and push any new files you’ve created.\n\n\n\nRecall that there are many different forms of evidence, and some sources can be trusted more than others. You should be clear what is trustworthy and what might be compromised by vested interests, political or business ideology; or are just unsubstantiated claims, deliberate misinformation, or clickbait.\n\nPeer reviewed and respectable papers are probably more trusted, but in some areas, it is also possible for a paper to be respected and respectable, and still come to false conclusions (especially in low quality or pay to publish journals) - note some papers have already been found in journals that were created by ChatGPT, and no one caught it! Authors can be biased (for example, if the author has something to gain from arguing a certain position) - consider who they work for, whether they have credibility, and whether they are likely independent and objective.\nGrey literature sources such as blogs and social media posts will be prevalent, take even greater care in taking the conclusions and argument at face value. Look closely and critically at their sources. Who are they, do they have appropriate knowledge and experience? From what you know about energy and GHG, do their numbers seem in a credible range, or even physically possible?\nGenerative AI results need to be looked at especially carefully (i.e. click through to the source of anything it links to and check how authoritative it is!)\n\n\n\n\n\nThere are webcams with microphones on several of the lab machines in B74 and B80 SAT that you can use for recording. The free/open source Audacity software is available for recording, editing and processing your audio files. The final output should be good quality mp3 (e.g. around 192Kbps or similar variable bitrate mp3).\nYou will likely find the lab environment too noisy for recording the final take of the audio. Most phones have reasonable quality voice recording software, or you can capture audio from your computer if you have a microphone.\n\n\nIf you are not comfortable recording your own voice, consider asking someone else not on the course who likes performing to read your script. You could also use text to speech, although this will likely negatively impact the final end product quality, unless it’s used sparingly or to make a particular point, e.g. voicing an argument from a machine point of view.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nConsider early in the process whether you want to record any interviews with anyone, e.g. experts on a particular topic.\n\n\nIf so, you will want to gain ethical approval that your methods are acceptable and leave time to schedule and record an interview, transcribe and summarise the results. Conversational podcasts with more than one speaker are normally more engaging.\n\nParticipant information sheet (to customise)\nEthics application form (to send to me by email) for approval\n\nEssentially, you just want to make it clear you’re going to record them for a podcast and include the audio in the podcast. Be clear that they can withdraw and have their audio removed if it is possible to do so. Be also clear whether they’re happy for the audio to be identifiable and for them to be identified (they might want to remain anonymous or have their words voiced by someone else or masked in some way). If there are limits, like they are happy for use as a coursework, but not in the public domain, then the participant information form is the place to do this.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should avoid including copyright content where possible.\nThere should be no more than 30% of the audio from sources other than you! Any clips from other sources should be short, e.g. no more than one or two sentences and should not be edited together to form the bulk of the content.\n\n\nIf you do choose to incorporate short sections of copyright material to illustrate positions or points from other sources e.g. short audio clips from online podcasts or talks, then you should not put your podcast in the public domain. You should also clearly state that it contains copyright materials in the accompanying notes that you submit in week 10. If the content, e.g. sound track, is rights free or needs attribution, then you should also put this in your week10-notes.md markdown file.\n\n\n\n\n\nThe podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week6/week6-task.html#task-1-select-a-research-question",
    "href": "week6/week6-task.html#task-1-select-a-research-question",
    "title": "Week 6 worksheet",
    "section": "",
    "text": "The topic of your podcast should explore a contemporary topic or question in ICT and sustainability. This can be your evaluation of the evidence available, but should be backed up by evidence you have identified and evaluated.\nExample ideas (but not limited to) might be:\n\nShould we be worried by AI’s energy demand?\nWhat is driving growth in ICT and can it be stopped?\nWhat is the role of ICT in helping us integrate renewables?\nShould we trust climate change advice from a chatbot?\nDoes a footballer’s tweet use the same energy as a rocket?\nWhat is stopping my phone or PC lasting 10 years?\nCan ICT be sustainable?\n\nI very much expect you to come up with your own ideas. But a key aspect is that you can find enough sources of high quality material and evidence to support an argument in a podcast.\n\nYou should create week6-notes.md in your week6 folder. Add a heading # Research question. Provide your research question and a brief justification paragraph of why you think it will make an interesting podcast.\nAdd a heading # Supporting material. List links to sources you’ve identified which provide evidence for and against the central argument of your research question. Your goal is to provide a well reasoned and logical argument based on objective and rational analysis. Unsupported opinion, unevidenced rants, sales/ greenwash will not gain good marks!\nRemember to: add, commit and push any new files you’ve created.\n\n\n\nRecall that there are many different forms of evidence, and some sources can be trusted more than others. You should be clear what is trustworthy and what might be compromised by vested interests, political or business ideology; or are just unsubstantiated claims, deliberate misinformation, or clickbait.\n\nPeer reviewed and respectable papers are probably more trusted, but in some areas, it is also possible for a paper to be respected and respectable, and still come to false conclusions (especially in low quality or pay to publish journals) - note some papers have already been found in journals that were created by ChatGPT, and no one caught it! Authors can be biased (for example, if the author has something to gain from arguing a certain position) - consider who they work for, whether they have credibility, and whether they are likely independent and objective.\nGrey literature sources such as blogs and social media posts will be prevalent, take even greater care in taking the conclusions and argument at face value. Look closely and critically at their sources. Who are they, do they have appropriate knowledge and experience? From what you know about energy and GHG, do their numbers seem in a credible range, or even physically possible?\nGenerative AI results need to be looked at especially carefully (i.e. click through to the source of anything it links to and check how authoritative it is!)"
  },
  {
    "objectID": "week6/week6-task.html#recording-advice-and-interviews",
    "href": "week6/week6-task.html#recording-advice-and-interviews",
    "title": "Week 6 worksheet",
    "section": "",
    "text": "There are webcams with microphones on several of the lab machines in B74 and B80 SAT that you can use for recording. The free/open source Audacity software is available for recording, editing and processing your audio files. The final output should be good quality mp3 (e.g. around 192Kbps or similar variable bitrate mp3).\nYou will likely find the lab environment too noisy for recording the final take of the audio. Most phones have reasonable quality voice recording software, or you can capture audio from your computer if you have a microphone.\n\n\nIf you are not comfortable recording your own voice, consider asking someone else not on the course who likes performing to read your script. You could also use text to speech, although this will likely negatively impact the final end product quality, unless it’s used sparingly or to make a particular point, e.g. voicing an argument from a machine point of view.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nConsider early in the process whether you want to record any interviews with anyone, e.g. experts on a particular topic.\n\n\nIf so, you will want to gain ethical approval that your methods are acceptable and leave time to schedule and record an interview, transcribe and summarise the results. Conversational podcasts with more than one speaker are normally more engaging.\n\nParticipant information sheet (to customise)\nEthics application form (to send to me by email) for approval\n\nEssentially, you just want to make it clear you’re going to record them for a podcast and include the audio in the podcast. Be clear that they can withdraw and have their audio removed if it is possible to do so. Be also clear whether they’re happy for the audio to be identifiable and for them to be identified (they might want to remain anonymous or have their words voiced by someone else or masked in some way). If there are limits, like they are happy for use as a coursework, but not in the public domain, then the participant information form is the place to do this.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should avoid including copyright content where possible.\nThere should be no more than 30% of the audio from sources other than you! Any clips from other sources should be short, e.g. no more than one or two sentences and should not be edited together to form the bulk of the content.\n\n\nIf you do choose to incorporate short sections of copyright material to illustrate positions or points from other sources e.g. short audio clips from online podcasts or talks, then you should not put your podcast in the public domain. You should also clearly state that it contains copyright materials in the accompanying notes that you submit in week 10. If the content, e.g. sound track, is rights free or needs attribution, then you should also put this in your week10-notes.md markdown file."
  },
  {
    "objectID": "week6/week6-task.html#learning-outcomes",
    "href": "week6/week6-task.html#learning-outcomes",
    "title": "Week 6 worksheet",
    "section": "",
    "text": "The podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week1/git_repo_setup.html",
    "href": "week1/git_repo_setup.html",
    "title": "Part 1: Setting up your version controlled repo!",
    "section": "",
    "text": "This is an absolute essential. Submission of all work, and evidence of your achievement, will be through a ‘portfolio submission’ via SCC’s gitlab instance https://scc-source.lancs.ac.uk (this will be private to you, so you must invite ‘@friday’ as a collaborator - please choose anything higher permission than ‘Guest’, e.g. ‘Developer’ otherwise I can’t see your files! ).\nTo set this up, go to https://scc-source.lancs.ac.uk and login to create an account if you don’t already have one.\n\nRead about the basics of version control\nYou may find this step by step tutorial also useful, if you haven’t used git from the Linux terminal before\n\nEssentially, the idea is that you ‘check out’ (normally) the latest version of a project from the server to your local machine. Work on the content (mainly markdown text files ending in ‘.md’ in this case). Add the new files and Commit your changes (with a helpful message explaining what you’ve done is a key thing). Then push a copy of your repo to the server with ‘git push’.\nThe great thing is, there’s a backed up version of not just your work, but each important step leading up to where you are now. This will be important for the overall assessment of your work.\nIt’ll take a while to ‘get comfy’ with how this works in practice. Although I think this is important general learning (to use git from the terminal). You might find it easier to use the built in git client in vscode.\nRepos can generally be public or private. You can invite collaborators to either. We’d ask that all your coursework remains private to avoid unwanted code sharing.\nCreate a repo and clone it to your file system. Download my template (ZIP), expand it, and copy the files and weekly subfolders into your folder you’ve ‘cloned’ from the server. You should have a structure where there is a folder for each week’s lab that’s committed and pushed to your scc-source account by the end of this lab.\n\ni.e. Your repo in this case should be private.\n\n\n\n\nYou must use either HTTPS or SSH based URLs for securely pushing/pulling your code. HTTPS is easier, since SSH would require that you generate a public/private key pair then upload your public key to gitlab as part of your profile.\nIt’s easier to generate a ‘personal access token’. You will need your token to have both read_repository and write_repository access rights.\n\nYou use a personal access token instead of a password. You’ll want to keep this somewhere safe for future use, as once created you can’t see it again. Don’t share it with anyone!\nThe good news is VS Code will remember your credentials for you. The git password cache will enable the password to be remembered from the terminal so you don’t have to enter your password or token every time you push/pull from the remote repo (git config --global credential.helper cache) from the shell.\n\n\n\n\ngenerate a public/private key pair then upload your public key to gitlab as part of your profile\nEnsure you understand how to use the SSH URIs (that start with git@ instead of HTTPS) with your key pair for cloning and pushing your repos from/to your gitlab repo."
  },
  {
    "objectID": "week1/git_repo_setup.html#a-word-on-security",
    "href": "week1/git_repo_setup.html#a-word-on-security",
    "title": "Part 1: Setting up your version controlled repo!",
    "section": "",
    "text": "You must use either HTTPS or SSH based URLs for securely pushing/pulling your code. HTTPS is easier, since SSH would require that you generate a public/private key pair then upload your public key to gitlab as part of your profile.\nIt’s easier to generate a ‘personal access token’. You will need your token to have both read_repository and write_repository access rights.\n\nYou use a personal access token instead of a password. You’ll want to keep this somewhere safe for future use, as once created you can’t see it again. Don’t share it with anyone!\nThe good news is VS Code will remember your credentials for you. The git password cache will enable the password to be remembered from the terminal so you don’t have to enter your password or token every time you push/pull from the remote repo (git config --global credential.helper cache) from the shell."
  },
  {
    "objectID": "week1/git_repo_setup.html#more-version-control",
    "href": "week1/git_repo_setup.html#more-version-control",
    "title": "Part 1: Setting up your version controlled repo!",
    "section": "",
    "text": "generate a public/private key pair then upload your public key to gitlab as part of your profile\nEnsure you understand how to use the SSH URIs (that start with git@ instead of HTTPS) with your key pair for cloning and pushing your repos from/to your gitlab repo."
  },
  {
    "objectID": "week9/week9-task.html",
    "href": "week9/week9-task.html",
    "title": "Week 9 worksheet",
    "section": "",
    "text": "As you know, weeks 6-10 are to be used to produce a 10 minute audio podcast exploring interesting and contemporary digital sustainability research questions. You should be moving to using the labs for editing your audio and getting help and opinions on whether the podcast you’re putting together meets the required standard.\nSome of you are catching up as the deadline approaches, it’s fine to use the labs to ask questions about past weeks’ tasks.\n\n\n\n\n\n\nReminder on AI use policy\n\n\n\nAmber: See AI and academic practice framework. AI can be used to help you search for evidence sources and explore opinions on topics. AI should not be used to generate the script or podcast audio.\n\n\n\n\nYou should have selected the topic of your podcast and found evidence for arguing your position. You should have a script with an index of how you plan to present this as an informative and engaging podcast. If you are planning to interview someone, having backing music, or find snippets of audio from other interviews and podcasts, this is the week to do it.\nToday’s goal is to create the audio assets for your podcast.\n\nPlease take careful note of where you got your audio from (source, start/end times, copyright), and if there are any licensing restrictions on its use.\nNote: if you are doing your own interviews, then ensure you note any restrictions as to how the audio is used (does it need to be disguised using a filter to mask the speaker’s voice - or do they need to be revoiced by an actor/you, can it be aired to other students this or next year, can it be made public)\nYou could also think about royalty free background, intro or outro music to incorporate. There is no need for a message from your sponsor :)\n\n\nYou should create week9-notes.md in your week9 folder.\nTake the podcast script from last week, copy it into your notes and add further notes for each audio clip you create, especially if taken from an external source or interview.\nBy the end you should have checked off each of the items in your ‘shooting list’ and have noted down how long each clip is. You should have a good idea of what the total running time of your podcast will be by the end of the task.\nI recommend storing the audio clips in your H drive or onedrive, and just adding links to them in your markdown document - this will avoid you having large binary files in your git repository!\nRemember to: add, commit and push any new (non-audio) files you’ve created.\n\nNote again: this is your framing of your podcast, and using one of the online gen AI podcast audio creation tools is not acceptable and is academic malpractice.\n\n\n\nAudacity is a free open source tool that you can use to record and edit your audio. There are also webcams with microphones in B74 and B80 you can use.\nKeys to making good audio recordings:\n\nmake a test recording to ensure the volume levels are acceptable\npay particular attention to background noise from people, vehicles or airconditioners. You will likely want to record out of hours in the lab with the aircon off, or in private outside the lab to ensure this. This is hard to remove in post-production.\nOverdriven audio (too loud) will ‘clip’ and sound distorted and be unpleasant to listen to.\nQuiet audio can be amplified and ‘normalised’, but this will also raise the noise floor, so it’s better to have a good recording in the first place.\n\n\n\n\n\nThe podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week9/week9-task.html#task-4-record-and-prepare-the-audio",
    "href": "week9/week9-task.html#task-4-record-and-prepare-the-audio",
    "title": "Week 9 worksheet",
    "section": "",
    "text": "You should have selected the topic of your podcast and found evidence for arguing your position. You should have a script with an index of how you plan to present this as an informative and engaging podcast. If you are planning to interview someone, having backing music, or find snippets of audio from other interviews and podcasts, this is the week to do it.\nToday’s goal is to create the audio assets for your podcast.\n\nPlease take careful note of where you got your audio from (source, start/end times, copyright), and if there are any licensing restrictions on its use.\nNote: if you are doing your own interviews, then ensure you note any restrictions as to how the audio is used (does it need to be disguised using a filter to mask the speaker’s voice - or do they need to be revoiced by an actor/you, can it be aired to other students this or next year, can it be made public)\nYou could also think about royalty free background, intro or outro music to incorporate. There is no need for a message from your sponsor :)\n\n\nYou should create week9-notes.md in your week9 folder.\nTake the podcast script from last week, copy it into your notes and add further notes for each audio clip you create, especially if taken from an external source or interview.\nBy the end you should have checked off each of the items in your ‘shooting list’ and have noted down how long each clip is. You should have a good idea of what the total running time of your podcast will be by the end of the task.\nI recommend storing the audio clips in your H drive or onedrive, and just adding links to them in your markdown document - this will avoid you having large binary files in your git repository!\nRemember to: add, commit and push any new (non-audio) files you’ve created.\n\nNote again: this is your framing of your podcast, and using one of the online gen AI podcast audio creation tools is not acceptable and is academic malpractice."
  },
  {
    "objectID": "week9/week9-task.html#making-a-good-recording",
    "href": "week9/week9-task.html#making-a-good-recording",
    "title": "Week 9 worksheet",
    "section": "",
    "text": "Audacity is a free open source tool that you can use to record and edit your audio. There are also webcams with microphones in B74 and B80 you can use.\nKeys to making good audio recordings:\n\nmake a test recording to ensure the volume levels are acceptable\npay particular attention to background noise from people, vehicles or airconditioners. You will likely want to record out of hours in the lab with the aircon off, or in private outside the lab to ensure this. This is hard to remove in post-production.\nOverdriven audio (too loud) will ‘clip’ and sound distorted and be unpleasant to listen to.\nQuiet audio can be amplified and ‘normalised’, but this will also raise the noise floor, so it’s better to have a good recording in the first place."
  },
  {
    "objectID": "week9/week9-task.html#learning-outcomes",
    "href": "week9/week9-task.html#learning-outcomes",
    "title": "Week 9 worksheet",
    "section": "",
    "text": "The podcast task is about critical understanding of evidence for and against a particular position w.r.t. ICT sustainability\nYou will learn how to find and evaluate reliable data to argue your position\nWhat makes a reliable source\nHow to quickly skim read and evaluate a written argument concerning ICT and sustainability"
  },
  {
    "objectID": "week2/week2-task.html",
    "href": "week2/week2-task.html",
    "title": "Week 2 worksheet",
    "section": "",
    "text": "The goal of today is to explore your own use and energy data demand of the digital infrastructures and appliances you use, and put this into contrast with other household appliances. The focus today is on direct energy and how this links to use patterns.\n\n\nSpend a few minutes writing a list of the digital and electronic technology you most use in the home. You should aim for 3-5 technology enabled appliances (i.e. with a digital processor and/or Internet or WiFi connected). This could include smart TVs, computers, games consoles, Wifi routers, smart speakers, etc.\n\nTake a photo of it to remind yourself of your inventory\nIf possible / safe to do so, take a picture of the label which explains it’s energy use requirement in Watts (this might be listed on a power adapter in some cases)\nWrite down the item, make notes of any attached speaker systems, external hardrives or other items that are needed to enable it to work (e.g. a PC will normally have an external monitor, maybe speakers, and so on)\n\nYou will need this to help you with the in lab task this week and next.\n\n\n\nTime available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available (git clone or git pull as needed to ensure you are up to date with the server). In the ‘week2’ folder create a new markdown document called username-week2-labnotes.md with your markdown editor of choice.\n\nFor each photo, and while you have time in the lab, put a subheading that describes it\nAdd a little detail on how often you use it and for how long (estimate hours per day, week and year)\nUsing your photos and the resources below, look up how much active power the device uses when in use and add this to a markdown table under the appropriate heading\nIf the device has a different power level when not in use (e.g. on standby or 0W if powered off when not in use), find out what this is and add it to the table\nFinally, add a row that calculates the number of kilowatt/hours (divide total in Watts by 1,000 and multiply by the number of hours of use) based on your estimate of use per day, week and year. For example, if you had a smart speaker that takes 5W all year round this would be:\n\n5×365×241000=43.8kWh\n\\frac{5 \\times 365 \\times 24}{1000} = 43.8kWh\n\n\ni.e. 365 days in a year by 24 hours per day to get total hours used, multiplied by the power demand in Watts to get Watt-hours\ndivide by 1000 to get kilowatt hours\n\n\nThe task should be time bounded, i.e. you should not spend much more than 40 minutes of concentrated time on this task. I would expect you to find and calculate the power for 3-5 assets in detail in this time.\nAdd a subsection ## Reflections with a short paragraph on your thoughts on how the various assets compare with one another. Which uses most energy, which uses least, how does this relate to use time or overall powered on time.\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task.\n\n\n\nIf you want to go further, you could try to approximate:\n\nthe amount of energy used by network traffic arising from your use of the Internet\nadd a further paragraph to your reflections discussing how this compares to the direct energy use of your device\nhow does this compare to a typical washing machine, refrigerator or domestic oven with the use pattern found in your household?\n\nWe will not worry about the cloud computing data centres, AI workloads or other energy demands that might be triggered by your device use for now.\n\n\n\n\nYou should have an appreciation of the direct energy requirements of the devices you use most often\nYou should appreciate their relative energy requirements and how this relates to use\nHow the impact of time affects the cumulative energy requirement (i.e. amount over a year)\nYou should be increasing your energy literacy\n\n\n\n\nHere are some useful digital resources to help. As before, very interested in which ones (or others) you actually use.\n\nComprehensive spreadsheet of typical UK appliance energy demands from gov.uk\nTypical UK home appliance energy demand summary sheet\nClimate Savers Computing Initiative on Wikipedia\nWhat appliances use the most electricity?\nEnergy ratings on energysavings trust\nWhat is a kilowatt/hour (kWh)? (YouTube)"
  },
  {
    "objectID": "week2/week2-task.html#startup-task-homework-prep",
    "href": "week2/week2-task.html#startup-task-homework-prep",
    "title": "Week 2 worksheet",
    "section": "",
    "text": "Spend a few minutes writing a list of the digital and electronic technology you most use in the home. You should aim for 3-5 technology enabled appliances (i.e. with a digital processor and/or Internet or WiFi connected). This could include smart TVs, computers, games consoles, Wifi routers, smart speakers, etc.\n\nTake a photo of it to remind yourself of your inventory\nIf possible / safe to do so, take a picture of the label which explains it’s energy use requirement in Watts (this might be listed on a power adapter in some cases)\nWrite down the item, make notes of any attached speaker systems, external hardrives or other items that are needed to enable it to work (e.g. a PC will normally have an external monitor, maybe speakers, and so on)\n\nYou will need this to help you with the in lab task this week and next."
  },
  {
    "objectID": "week2/week2-task.html#task",
    "href": "week2/week2-task.html#task",
    "title": "Week 2 worksheet",
    "section": "",
    "text": "Time available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available (git clone or git pull as needed to ensure you are up to date with the server). In the ‘week2’ folder create a new markdown document called username-week2-labnotes.md with your markdown editor of choice.\n\nFor each photo, and while you have time in the lab, put a subheading that describes it\nAdd a little detail on how often you use it and for how long (estimate hours per day, week and year)\nUsing your photos and the resources below, look up how much active power the device uses when in use and add this to a markdown table under the appropriate heading\nIf the device has a different power level when not in use (e.g. on standby or 0W if powered off when not in use), find out what this is and add it to the table\nFinally, add a row that calculates the number of kilowatt/hours (divide total in Watts by 1,000 and multiply by the number of hours of use) based on your estimate of use per day, week and year. For example, if you had a smart speaker that takes 5W all year round this would be:\n\n5×365×241000=43.8kWh\n\\frac{5 \\times 365 \\times 24}{1000} = 43.8kWh\n\n\ni.e. 365 days in a year by 24 hours per day to get total hours used, multiplied by the power demand in Watts to get Watt-hours\ndivide by 1000 to get kilowatt hours\n\n\nThe task should be time bounded, i.e. you should not spend much more than 40 minutes of concentrated time on this task. I would expect you to find and calculate the power for 3-5 assets in detail in this time.\nAdd a subsection ## Reflections with a short paragraph on your thoughts on how the various assets compare with one another. Which uses most energy, which uses least, how does this relate to use time or overall powered on time.\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task."
  },
  {
    "objectID": "week2/week2-task.html#extension-task",
    "href": "week2/week2-task.html#extension-task",
    "title": "Week 2 worksheet",
    "section": "",
    "text": "If you want to go further, you could try to approximate:\n\nthe amount of energy used by network traffic arising from your use of the Internet\nadd a further paragraph to your reflections discussing how this compares to the direct energy use of your device\nhow does this compare to a typical washing machine, refrigerator or domestic oven with the use pattern found in your household?\n\nWe will not worry about the cloud computing data centres, AI workloads or other energy demands that might be triggered by your device use for now."
  },
  {
    "objectID": "week2/week2-task.html#learning-outcomes",
    "href": "week2/week2-task.html#learning-outcomes",
    "title": "Week 2 worksheet",
    "section": "",
    "text": "You should have an appreciation of the direct energy requirements of the devices you use most often\nYou should appreciate their relative energy requirements and how this relates to use\nHow the impact of time affects the cumulative energy requirement (i.e. amount over a year)\nYou should be increasing your energy literacy"
  },
  {
    "objectID": "week2/week2-task.html#starting-points",
    "href": "week2/week2-task.html#starting-points",
    "title": "Week 2 worksheet",
    "section": "",
    "text": "Here are some useful digital resources to help. As before, very interested in which ones (or others) you actually use.\n\nComprehensive spreadsheet of typical UK appliance energy demands from gov.uk\nTypical UK home appliance energy demand summary sheet\nClimate Savers Computing Initiative on Wikipedia\nWhat appliances use the most electricity?\nEnergy ratings on energysavings trust\nWhat is a kilowatt/hour (kWh)? (YouTube)"
  },
  {
    "objectID": "week4/week4-task.html",
    "href": "week4/week4-task.html",
    "title": "Week 4 worksheet",
    "section": "",
    "text": "The goal of today is to learn how to take energy measurements associated with running specific software. We’ll be indirectly using the Intel RAPL energy reporting from the CPU in your lab machine. Note, this is specific to Intel processors, so will not work directly on anything else (e.g. a Mac with Apple Silicon, Google Chromebook etc.)\nIn short: I strongly recommend you do this on the lab machine, and not on your laptop.\nPlease scroll down to see running the stress-ng framework section before leaping in!.\n\n\nReally, we’re taking a snapshot of the energy use of the CPU from the time we start measuring to the time the task is complete.\n\nThis could catch any computation going on, not just your code, so we should aim for isolation, i.e. trying not to get data that’s confounded by other software and hardware running at the same time\nIdeally, recognise that the measurement framework may add additional overhead (hopefully in this case it should be relatively constant for all tests)\nUnderstanding how much is missed by what we are able to measure (for example, RAPL measures the CPU, but not the memory, I/O subsystems or anything else that makes up the total system energy)\nRepeatable and representative test selection. We should run tests more than once and take an average (arithmetic mean) and standard deviation. We should report each run and the average. I’d suggest running each test 5 times.\n\nNormally, test selection would involve picking representative use cases or data sets to ensure that data or interaction dependent behaviour is captured. In our case for this lab, we’re going to use a CPU stress testing framework to make the test development process simpler.\n\n\n\nTime available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available (git clone or git pull as needed to ensure you are up to date with the server), as before. In the ‘week4’ folder create a new markdown document called username-week4-labnotes.md, named with your user as in past weeks.\n\nRead about stress-ng a system stress testing framework\nSelect 3 tests that exercise the CPU (as that’s what we’re going to measure). There are a lot of options. In selecting your tests you really want a mixture of:\n\nduration (see --timeout)\nCPU intensity of workload (see --workload-load)\nnot too much that isn’t CPU (as that’s what we’re primarily measuring) (see --cpu) There are several examples in the stress-ng reference guide.\n\nIt’s easy to have a test that runs for too little, or too much time - so do a couple of test runs before taking your measurements.\nDownload this python script which we’ll use for energy reporting from Intel’s RAPL API. This is a python script and needs to be executable - you may need to change it’s file permissions (e.g. chmod u+x profiler.py in the terminal shell)\nIn your markdown report, add an introductory paragraph on what you plan to measure and your ‘method statement’ of how you’ve conducted your tests\nFor each test case, add a subsection with the goal of the test, the command line you’ve used and a table with the results.\nThe table should have the results of each test run, how much time it took and how many Joules of energy\nFinish the table with the average time taken and standard deviation. You could use a spreadsheet like excel, or a stats package like R (see getting started reference)\nFor each test case add a summary converting from Joules to Watts (e.g. using a converter, or by hand):\nRecall, the power P in watts (W) is equal to the energy E in joules (J), divided by the time period t in seconds (s) - take care with your units!!\nP=Et\nP = \\frac{E}{t}\n\nAdd a subsection called ## Reflections which summarises what you’ve found, and any personal observations on the number of Watt’s associated with the computations. How much variability was there between the tests; how does the energy compare with say a 12W LED, or watching TV for an hour?\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task.\n\n\n\nYou can download and build the stress-ng framework, e.g. using git clone, then build it using make. However, the easiest way to run the framework is as follows (using docker):\ndocker run --rm ghcr.io/colinianking/stress-ng --help\nThis will download the code and run it for you with the parameters --help in this case.\nTo run a test, pass in the configuration parameters to stress-ng.\ndocker run --rm ghcr.io/colinianking/stress-ng --cpu 1 --timeout 1m\nTo run a test and measure it’s energy, try passing the docker command to the profiler.py python script:\n./profiler.py \"docker run --rm ghcr.io/colinianking/stress-ng --cpu 1 --timeout 1m\"\nNote: there are double quotes around the parameter to the profiler script.\n\n\ndocker will download the code for you, but it will take time and resources to do so. You will want to run this once before you start doing your batches of 5 test energy measurement runs to avoid polluting your results datasets!\n\n\n\n\n\nYou should have an appreciation of how energy varies with computation\nShould know how to take measurements in a systematic way\nShould know how each run varies at a system level, and the challenges of apportioning to specific running software\nA ‘ready reckoning’ of the bounds for energy saving due to reducing load on the CPU\n\n\n\n\n\nA nice console based graph of your energy use (energygraph). Note, you do not need to (and should not) use sudo on our machines - we’ve opened up the RAPL permissions on the lab machines. Just ./energygraph if you’re in the folder where you’ve downloaded and built it."
  },
  {
    "objectID": "week4/week4-task.html#taking-good-measurements",
    "href": "week4/week4-task.html#taking-good-measurements",
    "title": "Week 4 worksheet",
    "section": "",
    "text": "Really, we’re taking a snapshot of the energy use of the CPU from the time we start measuring to the time the task is complete.\n\nThis could catch any computation going on, not just your code, so we should aim for isolation, i.e. trying not to get data that’s confounded by other software and hardware running at the same time\nIdeally, recognise that the measurement framework may add additional overhead (hopefully in this case it should be relatively constant for all tests)\nUnderstanding how much is missed by what we are able to measure (for example, RAPL measures the CPU, but not the memory, I/O subsystems or anything else that makes up the total system energy)\nRepeatable and representative test selection. We should run tests more than once and take an average (arithmetic mean) and standard deviation. We should report each run and the average. I’d suggest running each test 5 times.\n\nNormally, test selection would involve picking representative use cases or data sets to ensure that data or interaction dependent behaviour is captured. In our case for this lab, we’re going to use a CPU stress testing framework to make the test development process simpler."
  },
  {
    "objectID": "week4/week4-task.html#task",
    "href": "week4/week4-task.html#task",
    "title": "Week 4 worksheet",
    "section": "",
    "text": "Time available: 1 hour:\nMake sure you have an up to date copy of your coursework repo available (git clone or git pull as needed to ensure you are up to date with the server), as before. In the ‘week4’ folder create a new markdown document called username-week4-labnotes.md, named with your user as in past weeks.\n\nRead about stress-ng a system stress testing framework\nSelect 3 tests that exercise the CPU (as that’s what we’re going to measure). There are a lot of options. In selecting your tests you really want a mixture of:\n\nduration (see --timeout)\nCPU intensity of workload (see --workload-load)\nnot too much that isn’t CPU (as that’s what we’re primarily measuring) (see --cpu) There are several examples in the stress-ng reference guide.\n\nIt’s easy to have a test that runs for too little, or too much time - so do a couple of test runs before taking your measurements.\nDownload this python script which we’ll use for energy reporting from Intel’s RAPL API. This is a python script and needs to be executable - you may need to change it’s file permissions (e.g. chmod u+x profiler.py in the terminal shell)\nIn your markdown report, add an introductory paragraph on what you plan to measure and your ‘method statement’ of how you’ve conducted your tests\nFor each test case, add a subsection with the goal of the test, the command line you’ve used and a table with the results.\nThe table should have the results of each test run, how much time it took and how many Joules of energy\nFinish the table with the average time taken and standard deviation. You could use a spreadsheet like excel, or a stats package like R (see getting started reference)\nFor each test case add a summary converting from Joules to Watts (e.g. using a converter, or by hand):\nRecall, the power P in watts (W) is equal to the energy E in joules (J), divided by the time period t in seconds (s) - take care with your units!!\nP=Et\nP = \\frac{E}{t}\n\nAdd a subsection called ## Reflections which summarises what you’ve found, and any personal observations on the number of Watt’s associated with the computations. How much variability was there between the tests; how does the energy compare with say a 12W LED, or watching TV for an hour?\n\nDon’t forget to git add your new file, commit and push to the server at least at the end of the task."
  },
  {
    "objectID": "week4/week4-task.html#running-the-stress-ng-framework",
    "href": "week4/week4-task.html#running-the-stress-ng-framework",
    "title": "Week 4 worksheet",
    "section": "",
    "text": "You can download and build the stress-ng framework, e.g. using git clone, then build it using make. However, the easiest way to run the framework is as follows (using docker):\ndocker run --rm ghcr.io/colinianking/stress-ng --help\nThis will download the code and run it for you with the parameters --help in this case.\nTo run a test, pass in the configuration parameters to stress-ng.\ndocker run --rm ghcr.io/colinianking/stress-ng --cpu 1 --timeout 1m\nTo run a test and measure it’s energy, try passing the docker command to the profiler.py python script:\n./profiler.py \"docker run --rm ghcr.io/colinianking/stress-ng --cpu 1 --timeout 1m\"\nNote: there are double quotes around the parameter to the profiler script.\n\n\ndocker will download the code for you, but it will take time and resources to do so. You will want to run this once before you start doing your batches of 5 test energy measurement runs to avoid polluting your results datasets!"
  },
  {
    "objectID": "week4/week4-task.html#learning-outcomes",
    "href": "week4/week4-task.html#learning-outcomes",
    "title": "Week 4 worksheet",
    "section": "",
    "text": "You should have an appreciation of how energy varies with computation\nShould know how to take measurements in a systematic way\nShould know how each run varies at a system level, and the challenges of apportioning to specific running software\nA ‘ready reckoning’ of the bounds for energy saving due to reducing load on the CPU"
  },
  {
    "objectID": "week4/week4-task.html#fun",
    "href": "week4/week4-task.html#fun",
    "title": "Week 4 worksheet",
    "section": "",
    "text": "A nice console based graph of your energy use (energygraph). Note, you do not need to (and should not) use sudo on our machines - we’ve opened up the RAPL permissions on the lab machines. Just ./energygraph if you’re in the folder where you’ve downloaded and built it."
  }
]